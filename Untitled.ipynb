{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_tiling(feat_range, bins, offset):\n",
    "    \"\"\"\n",
    "    Create 1 tiling spec of 1 dimension(feature)\n",
    "    feat_range: feature range; example: [-1, 1]\n",
    "    bins: number of bins for that feature; example: 10\n",
    "    offset: offset for that feature; example: 0.2\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.linspace(feat_range[0], feat_range[1], bins+1)[1:-1] + offset\n",
    "  \n",
    "feat_range = [0, 1.0]\n",
    "bins = 10\n",
    "offset = 0.2\n",
    "\n",
    "tiling_spec = create_tiling(feat_range, bins, offset)\n",
    "# array([0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tilings(feature_ranges, number_tilings, bins, offsets):\n",
    "        \"\"\"\n",
    "        feature_ranges: range of each feature; example: x: [-1, 1], y: [2, 5] -> [[-1, 1], [2, 5]]\n",
    "        number_tilings: number of tilings; example: 3 tilings\n",
    "        bins: bin size for each tiling and dimension; example: [[10, 10], [10, 10], [10, 10]]: 3 tilings * [x_bin, y_bin]\n",
    "        offsets: offset for each tiling and dimension; example: [[0, 0], [0.2, 1], [0.4, 1.5]]: 3 tilings * [x_offset, y_offset]\n",
    "        \"\"\"\n",
    "        tilings = []\n",
    "        # for each tiling\n",
    "        for tile_i in range(number_tilings):\n",
    "            tiling_bin = bins[tile_i]\n",
    "            tiling_offset = offsets[tile_i]\n",
    "\n",
    "            tiling = []\n",
    "            # for each feature dimension\n",
    "            for feat_i in range(len(feature_ranges)):\n",
    "                feat_range = feature_ranges[feat_i]\n",
    "                # tiling for 1 feature\n",
    "                feat_tiling = create_tiling(feat_range, tiling_bin[feat_i], tiling_offset[feat_i])\n",
    "                tiling.append(feat_tiling)\n",
    "            tilings.append(tiling)\n",
    "        return np.array(tilings)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "feature_ranges = [[-1, 1], [2, 5]]  # 2 features\n",
    "number_tilings = 3\n",
    "bins = [[10, 10], [10, 10], [10, 10]]  # each tiling has a 10*10 grid\n",
    "offsets = [[0, 0], [0.2, 1], [0.4, 1.5]]\n",
    "\n",
    "tilings = create_tilings(feature_ranges, number_tilings, bins, offsets)\n",
    "\n",
    "print(tilings.shape)  # # of tilings X features X bins\n",
    "\n",
    "# (3, 2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tile_coding(feature, tilings):\n",
    "        \"\"\"\n",
    "        feature: sample feature with multiple dimensions that need to be encoded; example: [0.1, 2.5], [-0.3, 2.0]\n",
    "        tilings: tilings with a few layers\n",
    "        return: the encoding for the feature on each layer\n",
    "        \"\"\"\n",
    "        num_dims = len(feature)\n",
    "        feat_codings = []\n",
    "        for tiling in tilings:\n",
    "            feat_coding = []\n",
    "            for i in range(num_dims):\n",
    "                feat_i = feature[i]\n",
    "                tiling_i = tiling[i]  # tiling on that dimension\n",
    "                coding_i = np.digitize(feat_i, tiling_i)\n",
    "                feat_coding.append(coding_i)\n",
    "            feat_codings.append(feat_coding)\n",
    "        return np.array(feat_codings)\n",
    "        \n",
    "feature = [0.1, 2.5]\n",
    "\n",
    "coding = get_tile_coding(feature, tilings)\n",
    "coding\n",
    "\n",
    "# array([[5, 1],\n",
    "#       [4, 0],\n",
    "#       [3, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QValueFunction:\n",
    "    \n",
    "        def __init__(self, tilings, actions, lr):\n",
    "            self.tilings = tilings\n",
    "            self.num_tilings = len(self.tilings)\n",
    "            self.actions = actions\n",
    "            self.lr = lr # / self.num_tilings  # learning rate equally assigned to each tiling\n",
    "            self.state_sizes = [tuple(len(splits)+1 for splits in tiling) for tiling in self.tilings]  # [(10, 10), (10, 10), (10, 10)]\n",
    "            self.q_tables = [np.zeros(shape=(state_size+(len(self.actions),))) for state_size in self.state_sizes]\n",
    "\n",
    "        def value(self, state, action):\n",
    "            state_codings = get_tile_coding(state, self.tilings)  # [[5, 1], [4, 0], [3, 0]] ...\n",
    "            action_idx = self.actions.index(action)\n",
    "\n",
    "            value = 0\n",
    "            for coding, q_table in zip(state_codings, self.q_tables):\n",
    "                value += q_table[tuple(coding)+(action_idx,)]\n",
    "            return value / self.num_tilings\n",
    "\n",
    "        def update(self, state, action, target):\n",
    "            state_codings = get_tile_coding(state, self.tilings)  # [[5, 1], [4, 0], [3, 0]] ...\n",
    "            action_idx = self.actions.index(action)\n",
    "\n",
    "            for coding, q_table in zip(state_codings, self.q_tables):\n",
    "                delta = target - q_table[tuple(coding)+(action_idx,)]\n",
    "                q_table[tuple(coding)+(action_idx,)] += self.lr * delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a44311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from networks import ActorCriticNetwork\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, alpha=0.0003, gamma=0.99, n_actions=2):\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.action = None\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        self.actor_critic = ActorCriticNetwork(n_actions=n_actions)\n",
    "\n",
    "        self.actor_critic.compile(optimizer=Adam(learning_rate=alpha))\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        _, probs = self.actor_critic(state)\n",
    "\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=probs)\n",
    "        action = action_probabilities.sample()\n",
    "        log_prob = action_probabilities.log_prob(action)\n",
    "        self.action = action\n",
    "\n",
    "        return action.numpy()[0]\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor_critic.save_weights(self.actor_critic.checkpoint_file)\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor_critic.load_weights(self.actor_critic.checkpoint_file)\n",
    "        \n",
    "    def learn(self, state, reward, state_, done):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32) # not fed to NN\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            state_value, probs = self.actor_critic(state)\n",
    "            state_value_, _ = self.actor_critic(state_)\n",
    "            state_value = tf.squeeze(state_value)\n",
    "            state_value_ = tf.squeeze(state_value_)\n",
    "\n",
    "            action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "            log_prob = action_probs.log_prob(self.action)\n",
    "\n",
    "            delta = reward + self.gamma*state_value_*(1-int(done)) - state_value\n",
    "            actor_loss = -log_prob*delta\n",
    "            critic_loss = delta**2\n",
    "            total_loss = actor_loss + critic_loss\n",
    "\n",
    "        gradient = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
    "        self.actor_critic.optimizer.apply_gradients(zip(\n",
    "            gradient, self.actor_critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904301d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class GenericNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions):\n",
    "        super(GenericNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        state = T.tensor(observation, dtype=T.float).to(self.device)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.pi = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.v = nn.Linear(self.fc2_dims, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        state = T.tensor(observation, dtype=T.float).to(self.device)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pi = self.pi(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        return pi, v\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, input_dims, gamma=0.99, n_actions=2,\n",
    "                 layer1_size=256, layer2_size=256, n_outputs=1):\n",
    "        self.gamma = gamma\n",
    "        self.log_probs = None\n",
    "        self.n_outputs = n_outputs\n",
    "        self.actor = GenericNetwork(alpha, input_dims, layer1_size,\n",
    "                                           layer2_size, n_actions=n_actions)\n",
    "        self.critic = GenericNetwork(beta, input_dims, layer1_size,\n",
    "                                            layer2_size, n_actions=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        mu, sigma  = self.actor.forward(observation)#.to(self.actor.device)\n",
    "        sigma = T.exp(sigma)\n",
    "        action_probs = T.distributions.Normal(mu, sigma)\n",
    "        probs = action_probs.sample(sample_shape=T.Size([self.n_outputs]))\n",
    "        self.log_probs = action_probs.log_prob(probs).to(self.actor.device)\n",
    "        action = T.tanh(probs)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def learn(self, state, reward, new_state, done):\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "\n",
    "        critic_value_ = self.critic.forward(new_state)\n",
    "        critic_value = self.critic.forward(state)\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n",
    "        delta = ((reward + self.gamma*critic_value_*(1-int(done))) - \\\n",
    "                                                                critic_value)\n",
    "\n",
    "        actor_loss = -self.log_probs * delta\n",
    "        critic_loss = delta**2\n",
    "\n",
    "        (actor_loss + critic_loss).backward()\n",
    "\n",
    "        self.actor.optimizer.step()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "class NewAgent(object):\n",
    "    def __init__(self, alpha, beta, input_dims, gamma=0.99, n_actions=2,\n",
    "                 layer1_size=256, layer2_size=256, n_outputs=1):\n",
    "        self.gamma = gamma\n",
    "        self.log_probs = None\n",
    "        self.n_outputs = n_outputs\n",
    "        self.actor_critic = ActorCriticNetwork(alpha, input_dims, layer1_size,\n",
    "                                        layer2_size, n_actions=n_actions)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        pi, v = self.actor_critic.forward(observation)\n",
    "\n",
    "        mu, sigma = pi\n",
    "        sigma = T.exp(sigma)\n",
    "        action_probs = T.distributions.Normal(mu, sigma)\n",
    "        probs = action_probs.sample(sample_shape=T.Size([self.n_outputs]))\n",
    "        self.log_probs = action_probs.log_prob(probs).to(self.actor_critic.device)\n",
    "        action = T.tanh(probs)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def learn(self, state, reward, new_state, done):\n",
    "        self.actor_critic.optimizer.zero_grad()\n",
    "\n",
    "        _, critic_value_ = self.actor_critic.forward(new_state)\n",
    "        _, critic_value = self.actor_critic.forward(state)\n",
    "\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.actor_critic.device)\n",
    "        delta = reward + self.gamma*critic_value_*(1-int(done)) - critic_value\n",
    "\n",
    "        actor_loss = -self.log_probs * delta\n",
    "        critic_loss = delta**2\n",
    "\n",
    "        (actor_loss + critic_loss).backward()\n",
    "\n",
    "        self.actor_critic.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0e6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e8ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('mahesh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f37738af3a94a2c3c52fcb7c3ac7a85228e1daa8d0a55ba72443cd08ee98df1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
